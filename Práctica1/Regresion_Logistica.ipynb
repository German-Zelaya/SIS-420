{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dv36o83xAD5"
      },
      "source": [
        "**Grupo:**\n",
        "* Vildoso Flores Dario Alejandro\n",
        "* De La Torre Zelaya Daniel German"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L0KhPHmJVNa"
      },
      "source": [
        "# Ejercicion de programación - Regresión Logistica\n",
        "\n",
        "En este ejercicio se implementa regresion logistica y se aplica a dos diferentes datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Eg6cIE74JVNd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from scipy import optimize\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t52vBnb_JVNe"
      },
      "source": [
        "## Regresion Logistica\n",
        "\n",
        "En esta parte del ejercicio, creará un modelo de regresión logística para predecir si un estudiante será admitido en una universidad. Suponga que es el administrador de un departamento universitario y desea determinar las posibilidades de admisión de cada solicitante en función de sus resultados en dos exámenes. Tiene datos históricos de solicitantes anteriores que puede usar como un conjunto de capacitación para la regresión logística. Para cada ejemplo de capacitación, se tiene las calificaciones del solicitante en dos exámenes y la decisión de admisión. Su tarea es crear un modelo de clasificación que calcule la probabilidad de admisión de un solicitante en función de los puntajes de esos dos exámenes.\n",
        "\n",
        "La siguiente celda cargará los datos y las etiquetas correspondientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "WVzUelTXJVNe",
        "outputId": "e1c60f3d-e56c-43c0-940f-26d0b27f01c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>138</td>\n",
              "      <td>62</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.127</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>82</td>\n",
              "      <td>31</td>\n",
              "      <td>125</td>\n",
              "      <td>38.2</td>\n",
              "      <td>0.233</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44.2</td>\n",
              "      <td>0.630</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>68</td>\n",
              "      <td>42</td>\n",
              "      <td>250</td>\n",
              "      <td>42.3</td>\n",
              "      <td>0.365</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>139</td>\n",
              "      <td>62</td>\n",
              "      <td>41</td>\n",
              "      <td>480</td>\n",
              "      <td>40.7</td>\n",
              "      <td>0.536</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>2</td>\n",
              "      <td>75</td>\n",
              "      <td>64</td>\n",
              "      <td>24</td>\n",
              "      <td>55</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0.370</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>8</td>\n",
              "      <td>179</td>\n",
              "      <td>72</td>\n",
              "      <td>42</td>\n",
              "      <td>130</td>\n",
              "      <td>32.7</td>\n",
              "      <td>0.719</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>6</td>\n",
              "      <td>85</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31.2</td>\n",
              "      <td>0.382</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>110</td>\n",
              "      <td>46</td>\n",
              "      <td>130</td>\n",
              "      <td>67.1</td>\n",
              "      <td>0.319</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>2</td>\n",
              "      <td>81</td>\n",
              "      <td>72</td>\n",
              "      <td>15</td>\n",
              "      <td>76</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.547</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0               2      138             62             35        0  33.6   \n",
              "1               0       84             82             31      125  38.2   \n",
              "2               0      145              0              0        0  44.2   \n",
              "3               0      135             68             42      250  42.3   \n",
              "4               1      139             62             41      480  40.7   \n",
              "...           ...      ...            ...            ...      ...   ...   \n",
              "1995            2       75             64             24       55  29.7   \n",
              "1996            8      179             72             42      130  32.7   \n",
              "1997            6       85             78              0        0  31.2   \n",
              "1998            0      129            110             46      130  67.1   \n",
              "1999            2       81             72             15       76  30.1   \n",
              "\n",
              "      DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                        0.127   47        1  \n",
              "1                        0.233   23        0  \n",
              "2                        0.630   31        1  \n",
              "3                        0.365   24        1  \n",
              "4                        0.536   21        0  \n",
              "...                        ...  ...      ...  \n",
              "1995                     0.370   33        0  \n",
              "1996                     0.719   36        1  \n",
              "1997                     0.382   42        0  \n",
              "1998                     0.319   26        1  \n",
              "1999                     0.547   25        0  \n",
              "\n",
              "[2000 rows x 9 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar datos utilizando la libreria pandas\n",
        "# Las columnas son de el numero de embarazos, glucosa, presion de sangre, grosor de piel en triceps, insulina, indice de masa corporal, funcion del pedigri de diabetes y edad\n",
        "# contiene la etiqueta que indica si el paciente fue diagnosticado con diabetes\n",
        "datos = pd.read_csv('DatosDiabetes.csv')\n",
        "data = datos.values\n",
        "datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UZevLEXOLQ5x",
        "outputId": "51b3f58f-892b-42a2-96e2-56d0d9dbd87c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0\n",
              "0     1.0\n",
              "1     0.0\n",
              "2     1.0\n",
              "3     1.0\n",
              "4     0.0\n",
              "...   ...\n",
              "1995  0.0\n",
              "1996  1.0\n",
              "1997  0.0\n",
              "1998  1.0\n",
              "1999  0.0\n",
              "\n",
              "[2000 rows x 1 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = data[:, 8]\n",
        "pd.DataFrame(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0JGpI-T8OC1y"
      },
      "outputs": [],
      "source": [
        "# Funcion de normalizacion para las columnas\n",
        "def featureNormalize(X):\n",
        "\n",
        "    X_norm = X.copy()\n",
        "    mu = np.zeros(X.shape[1])\n",
        "    sigma = np.zeros(X.shape[1])\n",
        "\n",
        "    mu = np.mean(X, axis = 0)\n",
        "    sigma = np.std(X, axis = 0)\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return X_norm, mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "T9c6ss4ZOIRY",
        "outputId": "f58bf909-3a98-4c1c-fed4-be97a72b9502"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.515394</td>\n",
              "      <td>0.524553</td>\n",
              "      <td>-0.372481</td>\n",
              "      <td>0.873645</td>\n",
              "      <td>-0.722016</td>\n",
              "      <td>0.172683</td>\n",
              "      <td>-1.063246</td>\n",
              "      <td>1.180424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.120495</td>\n",
              "      <td>-1.159756</td>\n",
              "      <td>0.670080</td>\n",
              "      <td>0.625186</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.737249</td>\n",
              "      <td>-0.735551</td>\n",
              "      <td>-0.856326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.120495</td>\n",
              "      <td>0.742890</td>\n",
              "      <td>-3.604422</td>\n",
              "      <td>-1.300374</td>\n",
              "      <td>-0.722016</td>\n",
              "      <td>1.473638</td>\n",
              "      <td>0.491759</td>\n",
              "      <td>-0.177409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.120495</td>\n",
              "      <td>0.430980</td>\n",
              "      <td>-0.059713</td>\n",
              "      <td>1.308449</td>\n",
              "      <td>1.527142</td>\n",
              "      <td>1.240448</td>\n",
              "      <td>-0.327478</td>\n",
              "      <td>-0.771462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.817945</td>\n",
              "      <td>0.555744</td>\n",
              "      <td>-0.372481</td>\n",
              "      <td>1.246334</td>\n",
              "      <td>3.596367</td>\n",
              "      <td>1.044077</td>\n",
              "      <td>0.201161</td>\n",
              "      <td>-1.026055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>-0.515394</td>\n",
              "      <td>-1.440474</td>\n",
              "      <td>-0.268225</td>\n",
              "      <td>0.190382</td>\n",
              "      <td>-0.227201</td>\n",
              "      <td>-0.305970</td>\n",
              "      <td>-0.312021</td>\n",
              "      <td>-0.007680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1.299907</td>\n",
              "      <td>1.803381</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>1.308449</td>\n",
              "      <td>0.447546</td>\n",
              "      <td>0.062225</td>\n",
              "      <td>0.766899</td>\n",
              "      <td>0.246914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>0.694807</td>\n",
              "      <td>-1.128565</td>\n",
              "      <td>0.461568</td>\n",
              "      <td>-1.300374</td>\n",
              "      <td>-0.722016</td>\n",
              "      <td>-0.121872</td>\n",
              "      <td>-0.274924</td>\n",
              "      <td>0.756101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>-1.120495</td>\n",
              "      <td>0.243835</td>\n",
              "      <td>2.129667</td>\n",
              "      <td>1.556908</td>\n",
              "      <td>0.447546</td>\n",
              "      <td>4.284191</td>\n",
              "      <td>-0.469686</td>\n",
              "      <td>-0.601732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>-0.515394</td>\n",
              "      <td>-1.253329</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>-0.368651</td>\n",
              "      <td>-0.038272</td>\n",
              "      <td>-0.256877</td>\n",
              "      <td>0.235167</td>\n",
              "      <td>-0.686597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0    -0.515394  0.524553 -0.372481  0.873645 -0.722016  0.172683 -1.063246   \n",
              "1    -1.120495 -1.159756  0.670080  0.625186  0.402563  0.737249 -0.735551   \n",
              "2    -1.120495  0.742890 -3.604422 -1.300374 -0.722016  1.473638  0.491759   \n",
              "3    -1.120495  0.430980 -0.059713  1.308449  1.527142  1.240448 -0.327478   \n",
              "4    -0.817945  0.555744 -0.372481  1.246334  3.596367  1.044077  0.201161   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995 -0.515394 -1.440474 -0.268225  0.190382 -0.227201 -0.305970 -0.312021   \n",
              "1996  1.299907  1.803381  0.148800  1.308449  0.447546  0.062225  0.766899   \n",
              "1997  0.694807 -1.128565  0.461568 -1.300374 -0.722016 -0.121872 -0.274924   \n",
              "1998 -1.120495  0.243835  2.129667  1.556908  0.447546  4.284191 -0.469686   \n",
              "1999 -0.515394 -1.253329  0.148800 -0.368651 -0.038272 -0.256877  0.235167   \n",
              "\n",
              "             7  \n",
              "0     1.180424  \n",
              "1    -0.856326  \n",
              "2    -0.177409  \n",
              "3    -0.771462  \n",
              "4    -1.026055  \n",
              "...        ...  \n",
              "1995 -0.007680  \n",
              "1996  0.246914  \n",
              "1997  0.756101  \n",
              "1998 -0.601732  \n",
              "1999 -0.686597  \n",
              "\n",
              "[2000 rows x 8 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, mu, sigma = featureNormalize(datos.values[:, :8])\n",
        "pd.DataFrame(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfpf9aEFJVNg"
      },
      "source": [
        "<a id=\"section1\"></a>\n",
        "### Implementacion\n",
        "\n",
        "#### Fución Sigmoidea\n",
        "\n",
        "La hipotesis para la regresión logistica se define como:\n",
        "\n",
        "$$ h_\\theta(x) = g(\\theta^T x)$$\n",
        "\n",
        "donde la función $g$ is la función sigmoidea. La función sigmoidea se define como:\n",
        "\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$.\n",
        "\n",
        "Los resultados que debe generar la funcion sigmoidea para valores positivos amplios de `x`, deben ser cercanos a 1, mientras que para valores negativos grandes, la sigmoide debe generar valores cercanos 0. La evaluacion de `sigmoid(0)` debe dar un resultado exacto de 0.5. Esta funcion tambien debe poder trabajar con vectores y matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pk9dV23xJVNh"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    # Calcula la sigmoide de una entrada z\n",
        "    # convierte la intrada a un arreglo numpy\n",
        "    z = np.array(z)\n",
        "\n",
        "    g = np.zeros(z.shape)\n",
        "\n",
        "    g = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRsBEhsWJVNh"
      },
      "source": [
        "Se calcula el valor de la sigmoide aplicando la funcion sigmoid con `z=0`, se debe obtener un resultado de 0.5. RE recomienda experimentar con otros valores de `z`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIPk61NLJVNh",
        "outputId": "31d7015b-86a4-4eeb-e184-2851bacdf790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "g( [0, 0.5, 1] ) =  [0.5        0.62245933 0.73105858]\n"
          ]
        }
      ],
      "source": [
        "# Prueba la implementacion de la funcion sigmoid\n",
        "z = [0, 0.5, 1]\n",
        "g = sigmoid(z)\n",
        "\n",
        "print('g(', z, ') = ', g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDl3_8hvJVNi"
      },
      "source": [
        "<a id=\"section2\"></a>\n",
        "#### Función de Costo y Gradiente\n",
        "\n",
        "Se implementa la funcion cost y gradient, para la regresión logistica. Antes de continuar es importante agregar el termino de intercepcion a X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ELtMkiYDJVNi",
        "outputId": "8509d6ca-75e1-4355-fc7c-1158c2018dc5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.515394</td>\n",
              "      <td>0.524553</td>\n",
              "      <td>-0.372481</td>\n",
              "      <td>0.873645</td>\n",
              "      <td>-0.722016</td>\n",
              "      <td>0.172683</td>\n",
              "      <td>-1.063246</td>\n",
              "      <td>1.180424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.120495</td>\n",
              "      <td>-1.159756</td>\n",
              "      <td>0.670080</td>\n",
              "      <td>0.625186</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.737249</td>\n",
              "      <td>-0.735551</td>\n",
              "      <td>-0.856326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.120495</td>\n",
              "      <td>0.742890</td>\n",
              "      <td>-3.604422</td>\n",
              "      <td>-1.300374</td>\n",
              "      <td>-0.722016</td>\n",
              "      <td>1.473638</td>\n",
              "      <td>0.491759</td>\n",
              "      <td>-0.177409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.120495</td>\n",
              "      <td>0.430980</td>\n",
              "      <td>-0.059713</td>\n",
              "      <td>1.308449</td>\n",
              "      <td>1.527142</td>\n",
              "      <td>1.240448</td>\n",
              "      <td>-0.327478</td>\n",
              "      <td>-0.771462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.817945</td>\n",
              "      <td>0.555744</td>\n",
              "      <td>-0.372481</td>\n",
              "      <td>1.246334</td>\n",
              "      <td>3.596367</td>\n",
              "      <td>1.044077</td>\n",
              "      <td>0.201161</td>\n",
              "      <td>-1.026055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.515394</td>\n",
              "      <td>-1.440474</td>\n",
              "      <td>-0.268225</td>\n",
              "      <td>0.190382</td>\n",
              "      <td>-0.227201</td>\n",
              "      <td>-0.305970</td>\n",
              "      <td>-0.312021</td>\n",
              "      <td>-0.007680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.299907</td>\n",
              "      <td>1.803381</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>1.308449</td>\n",
              "      <td>0.447546</td>\n",
              "      <td>0.062225</td>\n",
              "      <td>0.766899</td>\n",
              "      <td>0.246914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.694807</td>\n",
              "      <td>-1.128565</td>\n",
              "      <td>0.461568</td>\n",
              "      <td>-1.300374</td>\n",
              "      <td>-0.722016</td>\n",
              "      <td>-0.121872</td>\n",
              "      <td>-0.274924</td>\n",
              "      <td>0.756101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.120495</td>\n",
              "      <td>0.243835</td>\n",
              "      <td>2.129667</td>\n",
              "      <td>1.556908</td>\n",
              "      <td>0.447546</td>\n",
              "      <td>4.284191</td>\n",
              "      <td>-0.469686</td>\n",
              "      <td>-0.601732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.515394</td>\n",
              "      <td>-1.253329</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>-0.368651</td>\n",
              "      <td>-0.038272</td>\n",
              "      <td>-0.256877</td>\n",
              "      <td>0.235167</td>\n",
              "      <td>-0.686597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6  \\\n",
              "0     1.0 -0.515394  0.524553 -0.372481  0.873645 -0.722016  0.172683   \n",
              "1     1.0 -1.120495 -1.159756  0.670080  0.625186  0.402563  0.737249   \n",
              "2     1.0 -1.120495  0.742890 -3.604422 -1.300374 -0.722016  1.473638   \n",
              "3     1.0 -1.120495  0.430980 -0.059713  1.308449  1.527142  1.240448   \n",
              "4     1.0 -0.817945  0.555744 -0.372481  1.246334  3.596367  1.044077   \n",
              "...   ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  1.0 -0.515394 -1.440474 -0.268225  0.190382 -0.227201 -0.305970   \n",
              "1996  1.0  1.299907  1.803381  0.148800  1.308449  0.447546  0.062225   \n",
              "1997  1.0  0.694807 -1.128565  0.461568 -1.300374 -0.722016 -0.121872   \n",
              "1998  1.0 -1.120495  0.243835  2.129667  1.556908  0.447546  4.284191   \n",
              "1999  1.0 -0.515394 -1.253329  0.148800 -0.368651 -0.038272 -0.256877   \n",
              "\n",
              "             7         8  \n",
              "0    -1.063246  1.180424  \n",
              "1    -0.735551 -0.856326  \n",
              "2     0.491759 -0.177409  \n",
              "3    -0.327478 -0.771462  \n",
              "4     0.201161 -1.026055  \n",
              "...        ...       ...  \n",
              "1995 -0.312021 -0.007680  \n",
              "1996  0.766899  0.246914  \n",
              "1997 -0.274924  0.756101  \n",
              "1998 -0.469686 -0.601732  \n",
              "1999  0.235167 -0.686597  \n",
              "\n",
              "[2000 rows x 9 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configurar la matriz adecuadamente, y agregar una columna de unos que corresponde al termino de intercepción.\n",
        "m, n = X.shape\n",
        "# Agraga el termino de intercepción a A\n",
        "X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "pd.DataFrame(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dREHB51mJVNi"
      },
      "source": [
        "La funcion de costo en una regresión logistica es:\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$\n",
        "\n",
        "y el gradiente del costo es un vector de la misma longitud como $\\theta$ donde el elemento $j^{th}$ (para $j = 0, 1, \\cdots , n$) se define como:\n",
        "\n",
        "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$\n",
        "\n",
        "Si bien este gradiente parece idéntico al gradiente de regresión lineal, la fórmula es diferente porque la regresión lineal y logística tienen diferentes definiciones de $h_\\theta(x)$.\n",
        "<a id=\"costFunction\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iqDZRIgjJVNi"
      },
      "outputs": [],
      "source": [
        "def calcularCosto(theta, X, y):\n",
        "    # Inicializar algunos valores utiles\n",
        "    m = y.size  # numero de ejemplos de entrenamiento\n",
        "\n",
        "    J = 0\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7G4SjTrwJVNi"
      },
      "outputs": [],
      "source": [
        "def descensoGradiente(theta, X, y, alpha, num_iters):\n",
        "    # Inicializa algunos valores\n",
        "    m = y.shape[0] # numero de ejemplos de entrenamiento\n",
        "\n",
        "    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente\n",
        "    theta = theta.copy()\n",
        "    J_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = sigmoid(X.dot(theta.T))\n",
        "        theta = theta - (alpha / m) * (h - y).dot(X)\n",
        "\n",
        "        J_history.append(calcularCosto(theta, X, y))\n",
        "    return theta, J_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "rBUlehg6JVNi",
        "outputId": "74ab7845-1b4a-4007-81d5-86976faaf2d4",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "theta calculado por el descenso por el gradiente: [-0.87971698  0.4082322   1.06593822 -0.18096292  0.00384399 -0.12762893\n",
            "  0.62672543  0.28393866  0.16147323]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGyCAYAAAAMKHu5AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAP0pJREFUeJzt3Ql8FPX9//FP7pBAQiAkEO5bbhARQSxVUPCm7a/F1hbqAUq9aT2oAtZSsfoXrUrF2lpQW6Far1bFAwUPEBQqyA1yXzkgd8i1O//H55vsZpckEEiys5t5PR+PcWdmZ2e/O4nZN99rwizLsgQAAMBBwu0uAAAAQKARgAAAgOMQgAAAgOMQgAAAgOMQgAAAgOMQgAAAgOMQgAAAgOMQgAAAgOMQgAAAgONE2l2AYOR2u+XQoUPSokULCQsLs7s4AACgDvTmFvn5+ZKWlibh4aeo47GCwDPPPGN17tzZiomJsc4991xr9erVtR47evRovXVHteWyyy7zHuN2u62ZM2dabdu2tWJjY60xY8ZY27dvr3N59u/fX+N7sLCwsLCwsEjQL/o9fiq21wAtWbJEpk+fLgsWLJDhw4fLk08+KePGjZNt27ZJSkpKteNff/11KS0t9W4fPXpUBg0aJD/+8Y+9+x599FF56qmnZNGiRdK1a1eZOXOmOefmzZslNjb2lGXSmh+1f/9+SUhIaLDPCgAAGk9eXp507NjR+z1+MmGagsRGGnqGDRsmzzzzjLf5SQt/2223yX333XfK12tgmjVrlhw+fFji4+NN9ZdWff3617+W3/zmN+aY3NxcSU1NlYULF8o111xTpwuYmJhoXkcAAgAgNJzO97etnaC1Jmft2rUyduzYqgKFh5vtVatW1ekcf/vb30yo0fCjdu/eLUeOHPE7p14MDVq1nbOkpMRcNN8FAAA0XbYGoKysLHG5XKZ2xpdua4g5lTVr1sjGjRvlxhtv9O7zvO50zjl37lwTkjyL1kABAICmK6SHwWvtz4ABA+Tcc8+t13lmzJhhqss8i/b9AQAATZetASg5OVkiIiIkPT3db79ut23b9qSvLSwslMWLF8sNN9zgt9/zutM5Z0xMjGkr9F0AAEDTZWsAio6OlqFDh8qyZcu8+7QTtG6PGDHipK999dVXTd+dn//85377ddSXBh3fc2qfntWrV5/ynAAAwBlsHwavQ+AnT54s55xzjmnK0lFdWrtz3XXXmecnTZok7du3N/10Tmz+mjBhgrRu3dpvv05ceOedd8qcOXOkZ8+e3mHwOjJMjwcAALA9AE2cOFEyMzPNUHbtpDx48GBZunSptxPzvn37qs3mqHMEff755/LBBx/UeM577rnHhKipU6dKTk6OjBo1ypyzLnMAAQCAps/2eYCCEfMAAQAQekJmHiAAAAA7EIAAAIDjEIAAAIDjEIAAAIDj2D4KzEn+vfaApOcXS5iEybTvd7e7OAAAOBYBKID+vHynfJdZKC1iIglAAADYiCawAIqPqcibhaXlwuwDAADYhwAUQPHRFQHIbYkUl7ntLg4AAI5FALKhBkgVlJTbWhYAAJyMABRA8TER3vVCAhAAALYhANlUA6T9gAAAgD0IQAHU3DcAlbhsLQsAAE5GAAqguGiawAAACAYEIJtqgOgEDQCAfQhANvUBKqIPEAAAtiEA2dQEVkAfIAAAbEMAsq0TNDVAAADYhQAUQAyDBwAgOBCAAogaIAAAggMByLZh8PQBAgDALgSgAKIGCACA4EAACiD6AAEAEBwIQAHULIph8AAABAMCUACFh4dJfGU/IJrAAACwDwHIpmawIgIQAAC2IQDZFIC4FxgAAPYhAAVYfExlE1ipSyzLsrs4AAA4EgEowOKjK2qAXG5LSsrddhcHAABHIgAFGHMBAQBgPwJQgMX5BSCGwgMAYAcCUIA1r+wDpJgMEQAAexCAAiy+sg+QogkMAAB7EIBsbAJjKDwAAPYgANnZBEYfIAAAbEEACjBuiAoAgP0IQAEWTx8gAABsRwCyswaIAAQAgC0IQDbdCsNzOwwAABB4BCAbZ4IuKKYGCAAAOxCA7AxANIEBAGALAlCANY+tCkD51AABAGALAlCAJcRGedfzi8tsLQsAAE5FAAqwmMhwiYoIM+vUAAEAYA8CUICFhYV5+wHRBwgAAHsQgGzQorIZjCYwAADsQQCyQYvKjtDaBGZZlt3FAQDAcQhANvA0gZW7LSkpd9tdHAAAHIcAZGMTmMqjGQwAgIAjANkggbmAAACwFQHI5skQuR0GAACBRwCysRO0ogYIAIDAIwDZ3AeIofAAAAQeAcjmG6JSAwQAQOARgOxuAmM2aAAAAo4AZANuiAoAgL0IQDagEzQAAPYiANmAYfAAANiLAGT3KLASmsAAAAg0ApANaAIDAMBeBCAbxEcTgAAAsBMByAYR4WHeuYAYBQYAQOARgGxuBqMGCACAwCMA2cRTA1TARIgAAAQcAcjmGqCiUpeUu9x2FwcAAEchAAXBUHhqgQAACCwCUBBMhkg/IAAAAosAZJMEAhAAALYhAAXBDVHzGAoPAICzAtD8+fOlS5cuEhsbK8OHD5c1a9ac9PicnBy55ZZbpF27dhITEyO9evWSd9991/v8gw8+KGFhYX7LWWedJcEmoVlVAMo9TgACACCQqtphbLBkyRKZPn26LFiwwISfJ598UsaNGyfbtm2TlJSUaseXlpbKxRdfbJ577bXXpH379rJ3715p2bKl33H9+vWTjz76yLsdGWnrx6wRAQgAAPvYmgzmzZsnU6ZMkeuuu85saxB655135IUXXpD77ruv2vG6/9ixY7Jy5UqJiqoIEFp7dCINPG3btpVglugTgPIIQAAAOKMJTGtz1q5dK2PHjq0qTHi42V61alWNr3n77bdlxIgRpgksNTVV+vfvLw8//LC4XC6/43bs2CFpaWnSrVs3ufbaa2Xfvn0nLUtJSYnk5eX5LYEMQNQAAQDgkACUlZVlgosGGV+6feTIkRpfs2vXLtP0pa/Tfj8zZ86Uxx9/XObMmeM9RpvSFi5cKEuXLpVnn31Wdu/eLRdccIHk5+fXWpa5c+dKYmKid+nYsaM0NmqAAACwT/B1jjkJt9tt+v/85S9/kYiICBk6dKgcPHhQHnvsMZk9e7Y55tJLL/UeP3DgQBOIOnfuLP/617/khhtuqPG8M2bMMH2RPLQGqLFDEDVAAAA4MAAlJyebEJOenu63X7dr67+jI7+074++zqNPnz6mxkib1KKjo6u9RjtI60ixnTt31loWHU2mSyARgAAAcGATmIYVrcFZtmyZXw2Pbms/n5qcf/75JsjocR7bt283waim8KMKCgrku+++M8cE473AFAEIAAAHzQOkzU7PP/+8LFq0SLZs2SLTpk2TwsJC76iwSZMmmeYpD31eR4HdcccdJvjoiDHtBK2doj1+85vfyIoVK2TPnj1mtNgPfvADU2P005/+VIJJVES4xEdX1GQRgAAAcFAfoIkTJ0pmZqbMmjXLNGMNHjzYdF72dIzW0Vs6MsxD++W8//77ctddd5n+PToPkIahe++913vMgQMHTNg5evSotGnTRkaNGiVffvmlWQ822gxWWOqS3OPcCgMAgEAKsyzLCug7hgDtBK2jwXJzcyUhIaHR3mf8k5/K1iP5Eh0ZLtvnVHXeBgAAjfv9bfutMJzM0xG6tNwtxWX+cxkBAIDGQwCyESPBAACwBwHIRgQgAADsQQCyETdEBQDAHgSgYKkBKiIAAQAQKAQgG9EEBgCAPQhANvK7IWoxAQgAgEAhANmIGiAAAOxBALIRnaABALAHAchG1AABAGAPAlCw9AEiAAEAEDAEIBslNKu6Fy01QAAABA4ByEYxkRESG1XxIyAAAQAQOAQgmyXFRZvHbCZCBAAgYAhAQRKAcopKxbIsu4sDAIAjEIBslhRf0RG6zGVJYanL7uIAAOAIBCCbtaysAVLZhaW2lgUAAKcgANmslW8AKiIAAQAQCAQgmyXFVc0FdIwaIAAAAoIAFERNYDmMBAMAICAIQDZrFU8TGAAAgUYAsllLnyYwOkEDABAYBKCgqgGiCQwAgEAgAAXJRIjqGE1gAAAEBAEoiJrAdDZoAADQ+AhANmseEylREWFmPbuQJjAAAAKBAGSzsLAw71B4RoEBABAYBKAgmgyRAAQAQGAQgIKoI3RxmVuOc0NUAAAaHQEoyEaCUQsEAEDjIwAFgSRmgwYAIKAIQEF2Q1RGggEA0PgIQEGAJjAAAAKLABQEaAIDACCwCEBBgCYwAAACiwAUZDVAxwpLbC0LAABOQAAKAsnxMd71o4U0gQEA0NgIQEGgdfOqGqCsAmqAAABobASgIBAfEynNoiLM+tECaoAAAGhsBKAgqwWiCQwAgMZHAAoSrZvHeIfBl7vcdhcHAIAmjQAUJJIrR4JZloYghsIDANCYCEBBgo7QAAAEDgEoSCRXNoEpOkIDANC4CEBB1gdIHWUyRAAAGhUBKEgk+zWBUQMEAEBjIgAFida+s0HTBwgAgEZFAArCTtD0AQIAoHERgIKwEzSjwAAAaFwEoCCRFBclYWEV61nMBg0AQKMiAAWJyIhwSYqrvB0GNUAAADQqAlAQaV05GzR9gAAAaFwEoCDsB3S8zCVFpeV2FwcAgCaLABSst8PIpxYIAIDGQgAK0pFgmfQDAgCg0RCAgnY2aAIQAACNhQAURFJaxHrXM/IJQAAANBYCUBBJSahqAsvIK7a1LAAANGUEoGCtAcqjBggAgMZCAArWGqB8aoAAAGgsBKAg0iouWiLDK+6HkU4NEAAAjYYAFETCw8OkTYuKWiA6QQMA0HgIQEEmpTIAHS0skXKX2+7iAADQJBGAgkxKQkVHaMvSuYCYDRoAgMZAAArSGiBFR2gAABoHASjIMBQeAIDGRwAKMqk+Q+HTqQECAKBREICCejZoaoAAAGiSAWj+/PnSpUsXiY2NleHDh8uaNWtOenxOTo7ccsst0q5dO4mJiZFevXrJu+++W69zBhPuBwYAQBMPQEuWLJHp06fL7NmzZd26dTJo0CAZN26cZGRk1Hh8aWmpXHzxxbJnzx557bXXZNu2bfL8889L+/btz/icwYb7gQEA0PjCLEsHXNtDa2eGDRsmzzzzjNl2u93SsWNHue222+S+++6rdvyCBQvksccek61bt0pUVFSDnLMmeXl5kpiYKLm5uZKQkCCB5HJb0vP+d8VtiQxonyj/uW1UQN8fAIBQdTrf37bVAGltztq1a2Xs2LFVhQkPN9urVq2q8TVvv/22jBgxwjSBpaamSv/+/eXhhx8Wl8t1xudUJSUl5qL5LnaJCA+T5Oae2aCpAQIAoDHYFoCysrJMcNEg40u3jxw5UuNrdu3aZZq+9HXa72fmzJny+OOPy5w5c874nGru3LkmMXoWrTGyU2rlZIiZ+cwGDQBAY4is64FPPfXUqU8WGSlt27aVUaNGSUpKijQ0bc7S8/7lL3+RiIgIGTp0qBw8eNA0i2mfnzM1Y8YM02/IQ2uA7AxB7RJj5duDuaYZTDtCp7VsZltZAABwdAB64okn6hRQjh49ah5ffvll+eEPf1jrscnJySbEpKen++3XbQ1RNdGRX9r3R1/n0adPH1O7o81fZ3JOpaPJdAkWGoA8DuceJwABAGBXE9ju3btPuezdu9fUnjzwwANy//33n/R80dHRpgZn2bJl3n0anHRb+/nU5Pzzz5edO3ea4zy2b99ugpGe70zOGYza+QSeQzn0AwIAIOj7AGmn48mTJ5v+OKeizU46jH3RokWyZcsWmTZtmhQWFsp1111nnp80aZJpnvLQ548dOyZ33HGHCT7vvPOO6QStnaLres5QcGINEAAAsKkJ7HTovDyZmZmnPG7ixInmuFmzZplmrMGDB8vSpUu9nZj37dtnApWH9st5//335a677pKBAwea99EwdO+999b5nKHAt8nrcC41QAAANKl5gIKVnfMAqQPZRTLqj5+Y9fH92sqCXwwNeBkAAAg1ITEPEE4+DD4srGKdJjAAABoeASgIRUWES0qLilFph2gCAwAgOPoA6WSDb775pulkrPr16ydXXXWV3/B01E+7xGaSnlciWQUlUlruluhIsioAALYFIB2Gfvnll8uBAwekd+/e3pmUtYOyjsrq3r17gxXOydJaxso3+0W0h1Z6XrF0bBVnd5EAAGgyTrta4fbbb5du3brJ/v37zd3WddHRWl27djXPoWG0TfCdC4h+QAAA2FoDtGLFCvnyyy+lVatW3n2tW7eWRx55xExUiIarAfJgKDwAADbXAOktI/Lz86vtLygoMDMxo+H6AHkcYiQYAAD2BqArrrhCpk6dKqtXrxadQkgXrRG6+eabTUdoNIx2vjVA3A4DAAB7A5DeFV47Ouu9tWJjY82iTV89evSQJ598smFL52BpPjVANIEBAGBzH6CWLVvKW2+9ZUaDeYbB6x3ZNQCh4bRpESOR4WFS7rbkIJ2gAQCwtwbooYcekqKiIhN4rrzySrPo+vHjx81zaBgR4WHee4LprTEAAICNAeh3v/ud6fB8Ig1F+hwaToekigCUX1wuuUVldhcHAADnBiDt9BzmuVGVj/Xr1/sNjUf9dUyqmvxwP7VAAAAEvg9QUlKSCT669OrVyy8E6a0xtFZIR4Kh4XRsVdURWpvB+rdPtLU8AAA4LgDpCC+t/bn++utNU5febt5D5//p0qWLGRmGhtPBtwboGB2hAQAIeACaPHmyedRbXuiw98jIM7qPKs6wBogmMAAAbOwD1KJFC+/wd6VD4idMmCC//e1vpbS0tAGLBr8+QMcIQAAA2BaAbrrpJtm+fbtZ37Vrl0ycOFHi4uLk1VdflXvuuafBCoaKuYBiIit+RAeyaQIDAMC2AKThZ/DgwWZdQ8/o0aPln//8pyxcuFD+/e9/N1jBIKajefvKofAagLQPFgAAsGkYvNvtNusfffSRXHbZZWa9Y8eOkpWV1fAldDhPM9jxMpdkFdDECACALQHonHPOkTlz5shLL70kK1askMsvv9zs3717t6SmpjZIoVCFjtAAAARBANLh8OvWrZNbb71V7r//fu89wF577TUZOXJkIxTR2XyHwtMPCACAhnHaY9kHDhwo3377bbX9jz32mERERDRQseDBSDAAABreGU/ms3btWu9w+L59+8rZZ5/dkOVCTU1gBCAAAOwJQBkZGWbou/b/admypdmXk5MjF154oSxevFjatGnTMCWD0bl1vHd9d1ahrWUBAMCxfYBuu+02c9+vTZs2ybFjx8yyceNGycvLk9tvv71xSulgic2ipHV8tFnfc5QABACALTVAS5cuNcPf+/Tp492nTWDz58+XSy65pEEKBX9dkuPlaGGppOeVSFFpucRFcxsSAAACWgOkcwBFRUVV26/7PPMDoWF18WkG25NFPyAAAAIegC666CK544475NChQ959Bw8elLvuukvGjBlT7wKhuq7JVSPB6AcEAIANAeiZZ54x/X26dOki3bt3N4veIV73Pf300w1QJNTUBOZBPyAAAOrvtDuT6C0vdCJE7Qe0detWs0/7A40dO7YBioNTNYFRAwQAQP1FnulNOi+++GKzIMA1QAQgAAAC1wT28ccfm9Fe2tR1otzcXOnXr5989tln9S8RqmkeEyltWsSYdZrAAAAIYADSe4BNmTJFEhISqj2XmJgoN910k8ybN68BioSadK1sBtM7wucXl9ldHAAAnBGA1q9fL+PHj6/1eZ0DSG+PgcbR1a8ZjKHwAAAEJAClp6fXOP+PR2RkpGRmZtarMKhbP6DdNIMBABCYANS+fXtzy4vabNiwQdq1a1e/0qBOcwHtyiywtSwAADgmAF122WUyc+ZMKS4urvbc8ePHZfbs2XLFFVc0dPlQqUdKc+/6zgwCEAAAARkG/8ADD8jrr78uvXr1kltvvVV69+5t9utcQHofMJfLJffff3+9CoOT3xU+MjxMyt0WAQgAgEAFoNTUVFm5cqVMmzZNZsyYIZZleecEGjdunAlBegwaR1REuOkIvSOjQHZlFkq5yy2REac9kTcAADjdiRA7d+4s7777rmRnZ8vOnTtNCOrZs6ckJSU1Xgnh1TO1uQlApS637DtWJN3aVDWLAQCARp4JWgPPsGHDzuSlqIceKS1E5IhZ1yBEAAIA4MzQhhJCetIRGgCABkEACiGMBAMAoGEQgEKIdoIOD6tY35GRb3dxAAAIWQSgEBIbFWGGw3tqgNzuipF4AADg9BCAQrQZrLjMLQdzjttdHAAAQhIBKIQ7Qm9PpxkMAIAzQQAKMb1SdSh8ha1HCEAAAJwJAlCI6dMuwbu+5XCerWUBACBUEYBCTLc28RJdeQsMAhAAAGeGABSC9wTTW2Ko3VmFUlzmsrtIAACEHAJQCDeD6Sj4bfQDAgDgtBGAQrwf0GaawQAAOG0EoBDUl47QAADUCwEoBBGAAACoHwJQCEqMi5K0xFizvvVwvlgWt8QAAOB0EIBCvB9Qfkm5HMjmlhgAAJwOAlAT6Ai96RDNYAAAnA4CUIjq374qAH17MMfWsgAAEGoIQCFqYIeW3vUNB3JtLQsAAKGGABSi2iXGSnLzGLO+fn8OHaEBADgNBKAQFRYWJoM6JJr1vOJy2XO0yO4iAQAQMghATaYZjH5AAADUFQEohA3sWFEDpNbvpx8QAAB1RQAKYYOoAQIA4IwQgEJYq/ho6diqmVnfeChXyl1uu4sEAEBICIoANH/+fOnSpYvExsbK8OHDZc2aNbUeu3DhQtMB2HfR1/n65S9/We2Y8ePHS1PuB1Rc5pYdGQV2FwcAgJBgewBasmSJTJ8+XWbPni3r1q2TQYMGybhx4yQjI6PW1yQkJMjhw4e9y969e6sdo4HH95hXXnlFmiLPSDD1v300gwEAEBIBaN68eTJlyhS57rrrpG/fvrJgwQKJi4uTF154odbXaI1O27ZtvUtqamq1Y2JiYvyOSUpKkqZoaOeqz/X13mO2lgUAgFBhawAqLS2VtWvXytixY6sKFB5utletWlXr6woKCqRz587SsWNHufrqq2XTpk3Vjlm+fLmkpKRI7969Zdq0aXL06NFaz1dSUiJ5eXl+S6jo3z5RoiMrfoxf78m2uzgAAIQEWwNQVlaWuFyuajU4un3kyJEaX6OBRmuH3nrrLXn55ZfF7XbLyJEj5cCBA37NXy+++KIsW7ZM/vjHP8qKFSvk0ksvNe9Vk7lz50piYqJ30WAVKmIiI7zNYPuOFUlGXrHdRQIAIOhFSogZMWKEWTw0/PTp00eee+45+f3vf2/2XXPNNd7nBwwYIAMHDpTu3bubWqExY8ZUO+eMGTNMPyQPrQEKpRB0TpdW8lVl7c/Xe7PlsgHt7C4SAABBzdYaoOTkZImIiJD09HS//bqt/XbqIioqSoYMGSI7d+6s9Zhu3bqZ96rtGO0vpB2rfZdQco5vPyCawQAACO4AFB0dLUOHDjVNVR7apKXbvrU8J6PNWt9++620a1d7rYc2j2kfoJMdE8roCA0AQIiNAtOmp+eff14WLVokW7ZsMR2WCwsLzagwNWnSJNNE5fHQQw/JBx98ILt27TLD5n/+85+bYfA33nijt4P03XffLV9++aXs2bPHhCntKN2jRw8zvL4pahkXLT1Tmpv1TYfypKi03O4iAQAQ1GzvAzRx4kTJzMyUWbNmmY7PgwcPlqVLl3o7Ru/bt8+MDPPIzs42w+b1WB3arjVIK1euNEPolTapbdiwwQSqnJwcSUtLk0suucT0D9KmrqZK+wHpRIgutyXf7MuRkT2S7S4SAABBK8yyLMvuQgQb7QSto8Fyc3NDpj/Q6+sOyPR/rTfrt4/pKdMv7mV3kQAACNrvb9ubwNAwzuvW2ru+6rssW8sCAECwIwA1EWktm0nX5HjvLTEKS+gHBABAbQhATcjI7hW1QOVuS77aw2gwAABqQwBqQkZ2r+r4vPK72m/9AQCA0xGAmpARlTVAaiX9gAAAqBUBqAlpFR8tfdsleOcDyikqtbtIAAAEJQJQE+0HpJMbfLmLZjAAAGpCAGpizveZAHHFdprBAACoCQGoCc4HFB1Z8WNdvi1DmOcSAIDqCEBNTLPoCBlROSni4dxi2Zaeb3eRAAAIOgSgJuiis1K86x9vzbC1LAAABCMCUBN0Ye+qALR8a6atZQEAIBgRgJqgTq3jpFubittirN2XLblFZXYXCQCAoEIAauK1QC63JZ/uoBYIAABfBCAH9ANatiXd1rIAABBsCEBN1LAuraRFTKRZX7Y1Q0rL3XYXCQCAoEEAaqJ0LqAxfSpqgfKLy7k3GAAAPghATdj4/u2860s3HrG1LAAABBMCUBM2ulcbaRYVYdY/2Jwu5S6awQAAUASgJj4r9IVntTHrxwpL5as92XYXCQCAoEAAclQz2GFbywIAQLAgADlgOLzn5qjvbTxi5gUCAMDpCEBNXPOYSPl+r4pmsIz8Eln13VG7iwQAgO0IQA7wgyHtveuv/++ArWUBACAYEIAc4MKzUqRFbMWkiO9vPCJFpeV2FwkAAFsRgBwgNipCrhhY0Rm6sNQlH27m1hgAAGcjADnEhMFVzWBv/O+grWUBAMBuBCAH3RusfctmZv2zHVmSkVdsd5EAALANAcghwsPDvJ2hdSj8q2vpDA0AcC4CkINMHNbRu774q33iZk4gAIBDEYAcpGOrOLmgZ7JZ33/suHy+kzvEAwCciQDkMNcO7+Rdf2XNPlvLAgCAXQhADjOmT6q0aRFj1nU4fEY+naEBAM5DAHKYqIhw+ck5Hcx6uduSxWv2210kAAACjgDkQNcM6yThYRXrL325V0rKXXYXCQCAgCIAObQz9Pj+bc16Zn6J/Gf9YbuLBABAQBGAHOqGUd2863/9bJdYFkPiAQDOQQByqKGdk2RIp5ZmfeuRfPli51G7iwQAQMAQgBzsRp9aoOc/22VrWQAACCQCkION65fqvT/Yiu2Z8u2BXLuLBABAQBCAHCwyIlxuHl1VC/TUxztsLQ8AAIFCAHK4nwzrKG0TYr0TI246RC0QAKDpIwA5XExkhF8t0NPLdtpaHgAAAoEABLnm3E6SUnl7jKWbjsjmQ3l2FwkAgEZFAILERkXITaO7e7cffX+rreUBAKCxEYDgvUu8Z0TY8m2Z8sXOLLuLBABAoyEAwVsL9OtLenm35763RdxuZocGADRNBCB4TRjcXvq2SzDrGw/myX82HLK7SAAANAoCELzCw8Pkt5f18W7/8b2tUlRabmuZAABoDAQg+BnVM1lG92pj1g/lFsvTHzMsHgDQ9BCAUM2DV/WT6Ihw753id2YU2F0kAAAaFAEI1XRNjpebKidHLHNZMuutjWJZdIgGADQdBCDU6Fff7yEdkiqGxa/87qi8+c1Bu4sEAECDIQChRs2iI+TBK/t5tx98e7Nk5BXbWiYAABoKAQi1Gts3Va4Y2M6s5x4vk9++8S1NYQCAJoEAhJN66Or+ktw82qx/tCVD3vgfTWEAgNBHAMJJtYqPljkTBni3H3x7kxzMOW5rmQAAqC8CEE5pfP+2cvXgNLOeV1wut7/yPylzue0uFgAAZ4wAhDp56Kr+3lFha/dmy7wPt9tdJAAAzhgBCHWSGBclT/90iESGh5ntZ5d/Jyu2Z9pdLAAAzggBCHU2pFOS3Dv+LO/2nYv/J/uPFdlaJgAAzgQBCKflxgu6ypizUsx6dlGZ3Ljoayko4YapAIDQQgDCaQkLC5N5EwdLtzbxZntber7cteQbcbuZHwgAEDoIQDhtic2i5K+TzpGE2Eiz/eHmdHnsg212FwsAgDojAOGMdGvTXJ752dlS2SfadIr++xe77S4WAAB1QgDCGfterzby4FVV9wv73X82y1vcNBUAEAIIQKiXSSO6yG0X9fBu//pf62X5tgxbywQAwKkQgFBv0y/uJT89t5NZL3dbctNLa+VT5ggCAASxoAhA8+fPly5dukhsbKwMHz5c1qxZU+uxCxcuNCORfBd9nS+9Y/msWbOkXbt20qxZMxk7dqzs2LEjAJ/EmfRnMGdCf7lsQFuzXVLulhtf/JqaIABA0LI9AC1ZskSmT58us2fPlnXr1smgQYNk3LhxkpFR+5dnQkKCHD582Lvs3bvX7/lHH31UnnrqKVmwYIGsXr1a4uPjzTmLi4sD8ImcKSI8TP50zRAZ368iBJWWu2Xqi2tl2ZZ0u4sGAEDwBaB58+bJlClT5LrrrpO+ffua0BIXFycvvPDCSWsc2rZt611SU1P9an+efPJJeeCBB+Tqq6+WgQMHyosvviiHDh2SN998M0CfypmiIsLl6Z8NkcsHtDPbpS63TH1prbz69X67iwYAQPAEoNLSUlm7dq1povIWKDzcbK9atarW1xUUFEjnzp2lY8eOJuRs2rTJ+9zu3bvlyJEjfudMTEw0TWu1nbOkpETy8vL8Fpx5CPrTNYPlykEVd493uS25+7UN8vSyHSacAgAgTg9AWVlZ4nK5/GpwlG5riKlJ7969Te3QW2+9JS+//LK43W4ZOXKkHDhwwDzved3pnHPu3LkmJHkWDVY4c5ER4fLkxMHyy5FdvPse/3C73P/mRilzuW0tGwAAQdEEdrpGjBghkyZNksGDB8vo0aPl9ddflzZt2shzzz13xuecMWOG5Obmepf9+2myaYg+QbOv7CszLq26eeo/V++Ta/+6WrIKSmwtGwAAtgag5ORkiYiIkPR0/46yuq19e+oiKipKhgwZIjt37jTbntedzjljYmJMx2rfBfWnfbVuGt3d1AZFR1T8qq3ZfUyufPpz2XAgx+7iAQAczNYAFB0dLUOHDpVly5Z592mTlm5rTU9daBPat99+a4a8q65du5qg43tO7dOjo8Hqek40rAlD2svim86TlBYxZvtwbrH834JV8tKXe+kXBABwZhOYDoF//vnnZdGiRbJlyxaZNm2aFBYWmlFhSpu7tInK46GHHpIPPvhAdu3aZYbN//znPzfD4G+88UZvrcOdd94pc+bMkbffftuEIz1HWlqaTJgwwbbP6XRnd0qS/942SoZ2TvIOk5/55kYzSuxYYandxQMAOEzF7bxtNHHiRMnMzDQTF2onZe3bs3TpUm8n5n379pmRYR7Z2dlm2Lwem5SUZGqQVq5caYbQe9xzzz0mRE2dOlVycnJk1KhR5pwnTpiIwEpJiJVXppwnf3hnsyxatdd7J/kNBz6Vx388WEb1TLa7iAAAhwizaIOoRpvMdDSYdoimP1Dj+Ghzutz92nrJLirz7pt4Tkf57WV9JDEuytayAQCa/ve37U1gcKaxfVNl6Z3fk1E9qmp9lny9X8Y+sUKWbjxsa9kAAE0fAQi2SU2IlRevP1d+P6G/NI+paI3NzC+Rm19eJ9cv/Ep2ZRbYXUQAQBNFAIKtwsPD5BfndZYP7vqejDkrxbv/460ZcskTn8qc/26W3ONVzWQAADQE+gDVgD5A9tBfxf9uOCx/eGeLHMmrunFt6/houeXCHvKz4Z0kNirC1jICAJrG9zcBqAYEIHsVlZbLghW75LkV30lJedWtM9omxMqtF/WQn5zTUaIjqbwEAPgjANUTASg4HMgukj8u3Sb/WX/Ib3+HpGZmhun/O7uDNIumRggAUIEAVE8EoOCy+VCezPtwu3y0xf/2Jq3io03/oUkjOkvr5hWzTAMAnCuPAFQ/BKDgtH5/jglCK7Zn+u2PiQyXH57dXn52bmcZ0CHRtvIBAOxFAKonAlBw23gwV57/bJfpMO1y+//6DmifaDpLXzUoTeIrh9YDAJwhjwBUPwSg0Okj9Pcv9sjiNfuksNTl95zOKzS+f1uZMLi9jOjeWiLCw2wrJwAgMAhA9UQACi0FJeXy9jeH5J9r9srGg3nVnte70F85KE2uHpxmaoj0hrkAgKaHAFRPBKDQteFAjvxz9T7TPKbB6ETtWzaTi/ummuXcrq0kKoLh9ADQVBCA6okAFPqKy1yybEuGvPXNQVm+LVNKXVXzCXkkxEbKhWelyEVnpcj5PZIlmZFkABDSCED1RABqWnKLyuTdjYflvY1HZNV3WVLmqvlXvk+7BLmgZ7IJQ+d2acUcQwAQYghA9UQAarryistkxbZM+XBzunyyLUPyi6s3k6noiHAZ2CFRzunSSs7pnCRDOydJUnx0wMsLAKg7AlA9EYCcobTcLV/vOSaf7cySz3dkycZDuXKy/xu6t4mXczq3ksGdWprO1L1SW3BLDgAIIgSgeiIAOdOxwlJZ+V2WfLEzS1Z+d1T2Hi066fFREWHSu20LE4b665JWEYpoOgMAexCA6okABJWRXyzr9mbL13uy5au92bLpYK6UnzDx4ol0hH3HpDjpldpceqS0MI8airq3aU4wAoBGRgCqJwIQanK81GWG2X97MFc2Hcozj99lFpy02cw3GOkQ/C6t46Vz6zjz2Mnz2CqOcAQADYAAVE8EINRVYUm5bDmc5w1FOzIKZGd6frWZqU8lNSFGOreKl7SWsZLWspm0a9lM0hIr1tMSm0lCs0gmcASABvz+5mZJQD3o/cbMSLEurbz73G5LDuUelx3pBbIjI1+262N6vuzKKqx11Fl6XolZahMXHVERjBJjJaVFrLRpESPJzaPNo1maVzwmNosiKAFAHVADVANqgNAY9H+1nKIy2XO00HSwrlgKzfa+Y0WSVVBa7/fQjtnJlWGoVXy0JMVFS8u4KO9jyzjdV7WtjxquCE0AmgJqgIAgpCFD5xLSZUinpGrPF5WWy6GcYjmUc1wO5x73WS82NUq6XlxWfUZrXzrJox6vS13pnEeJcVFmZuwWsVHSwjxGmhvK6nbFo2fx324eE2X6L2mI4rYiAEIJAQgIEnHRkdIjRUePNa+1Bin3eJlk5pdULAUnPOaXmFokfTxaWFKnztlKbxPieX19RIaHecOQfpZmURHebV03j5X7K9Yr9sdEhUtMZITERIabeZX0Ubc967FR4RId4TnOc0yERIRTawXgzBGAgBCqQdImLF16prY46bEud0VYyi4qlZyiUskurFj37MsuKvPbr01zevPYmm4gW1c6RYD2caro51S/MFXXwOUbiPQxMiLM1GjpY2R4uGkSNI+R4RIVHlaxP8Kzrs/7HGOeC6vcF27OX3FMxT4NXBFhYeYxXJ/Tx8ptrfzSdT1PeLic5LjKxed5z7pnv3n9CcfRRAk0PAIQ0ATpl6b2AdLldGhwKiytCDEFupSUSV7lutlXUuYNObroKLiiMpccLy2XolJ9dMnxMpd3vaab0DYUDVzlpa7KEXdl0tRpBtIgpRVfGog0Enm29VF3+G6bXb7blSFaA5Zn23Oc/6PnPU7Y9r6f59z+2xVlrFipeK+KR89+z77KPVWv8R5b+dow/2198JTd9/iqde+Rlcee+ty+ZagqY02vrQqe1c7jW44TAmq1uHrCDp8Se893steH1fP1Jx4Q1oDnDjvx+VOEdd+ntUn9F+d1FrsQgAD4BaeEWO0PFNUg5yt3uSsDUsViglFZRVjSpbjMJSXl7oqlrCIwlZRVbOutSkrKXbWsV2x7181rXFLusqTM7TaPp5q0MtRok6bLsqRigoWm9dngTB2SmhGAADRN2oSUoEsDBarToX2mNASVudymc7iGMc+2CUqe/e6q58t8A5Ruuz37K16rUxxoLZnLqpjuwOzTUOLZb57zPa5qf9VxWtPm9p6jtuO872dZJvzo59HYo8+73RWP4tnWc5mD/Lc9r/NuV14X3+dP+lh5fsYKoykiAAFokrQq3tN/B/VjwldFvqoMWJ5w5bPtPbaigsqzx/M6z4wrFetVz1ccW/k6s1n1Xr7vXVWWqmOqznfC+1d7/oTynLAtp3p/n+c95/Z/P/9r5bdd7VqesF1TbV61Y07vHDUF1urnsE76fPUyVD9p9c9+eu8ZG2XvDPgEIADASZl+NpV9NyKq9zABQhL/NAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI5DAAIAAI7D3eBrYFmWeczLy7O7KAAAoI4839ue7/GTIQDVID8/3zx27NjR7qIAAIAz+B5PTEw86TFhVl1iksO43W45dOiQtGjRQsLCwho8nWqw2r9/vyQkJDTouVGF6xwYXOfA4DoHBtc59K+1RhoNP2lpaRIefvJePtQA1UAvWocOHRr1PfQHzv9gjY/rHBhc58DgOgcG1zm0r/Wpan486AQNAAAchwAEAAAchwAUYDExMTJ79mzziMbDdQ4MrnNgcJ0Dg+vsrGtNJ2gAAOA41AABAADHIQABAADHIQABAADHIQABAADHIQAF0Pz586VLly4SGxsrw4cPlzVr1thdpKD26aefypVXXmlm9NQZud98802/57X//qxZs6Rdu3bSrFkzGTt2rOzYscPvmGPHjsm1115rJtpq2bKl3HDDDVJQUOB3zIYNG+SCCy4wPxedmfTRRx8Vp5g7d64MGzbMzHqekpIiEyZMkG3btvkdU1xcLLfccou0bt1amjdvLj/60Y8kPT3d75h9+/bJ5ZdfLnFxceY8d999t5SXl/sds3z5cjn77LPNqI8ePXrIwoULxUmeffZZGThwoHfitxEjRsh7773nfZ7r3DgeeeQR8/fjzjvv9O7jWtffgw8+aK6r73LWWWeF1jXWUWBofIsXL7aio6OtF154wdq0aZM1ZcoUq2XLllZ6errdRQta7777rnX//fdbr7/+uo5UtN544w2/5x955BErMTHRevPNN63169dbV111ldW1a1fr+PHj3mPGjx9vDRo0yPryyy+tzz77zOrRo4f105/+1Pt8bm6ulZqaal177bXWxo0brVdeecVq1qyZ9dxzz1lOMG7cOOvvf/+7+ezffPONddlll1mdOnWyCgoKvMfcfPPNVseOHa1ly5ZZX3/9tXXeeedZI0eO9D5fXl5u9e/f3xo7dqz1v//9z/zckpOTrRkzZniP2bVrlxUXF2dNnz7d2rx5s/X0009bERER1tKlSy2nePvtt6133nnH2r59u7Vt2zbrt7/9rRUVFWWuveI6N7w1a9ZYXbp0sQYOHGjdcccd3v1c6/qbPXu21a9fP+vw4cPeJTMzM6SuMQEoQM4991zrlltu8W67XC4rLS3Nmjt3rq3lChUnBiC32221bdvWeuyxx7z7cnJyrJiYGBNilP4Po6/76quvvMe89957VlhYmHXw4EGz/ec//9lKSkqySkpKvMfce++9Vu/evS0nysjIMNdsxYoV3muqX9Kvvvqq95gtW7aYY1atWmW29Q9XeHi4deTIEe8xzz77rJWQkOC9rvfcc4/5Y+lr4sSJJoA5mf7u/fWvf+U6N4L8/HyrZ8+e1ocffmiNHj3aG4C41g0XgAYNGlTjc6FyjWkCC4DS0lJZu3ataaLxvd+Ybq9atcrWsoWq3bt3y5EjR/yuqd7/RZsWPddUH7XZ65xzzvEeo8frtV+9erX3mO9973sSHR3tPWbcuHGmGSg7O1ucJjc31zy2atXKPOrvbVlZmd911mruTp06+V3nAQMGSGpqqt811Jsdbtq0yXuM7zk8xzj199/lcsnixYulsLDQNIVxnRueNr9o88qJ14Nr3XB27Nhhuih069bNdDXQJq1QusYEoADIysoyf/B8f9BKt/VLHKfPc91Odk31UduVfUVGRpovd99jajqH73s4hdvtNv0kzj//fOnfv7/3Gmg41CB5sut8qmtY2zH6x+748ePiFN9++63pD6H9GW6++WZ54403pG/fvlznBqbhct26daaP24m41g1j+PDhpj/O0qVLTf82/Uep9qXUO7GHyjXmbvAAvP9i3rhxo3z++ed2F6XJ6t27t3zzzTempu21116TyZMny4oVK+wuVpOyf/9+ueOOO+TDDz80AxvQOC699FLvunbu10DUuXNn+de//mUGpYQCaoACIDk5WSIiIqr1gNfttm3b2lauUOa5bie7pvqYkZHh97yOMNCRYb7H1HQO3/dwgltvvVX++9//yieffCIdOnTw7tdroE24OTk5J73Op7qGtR2jo6FC5Y9lQ9B/FetIlqFDh5raiUGDBsmf/vQnrnMD0uYX/f9eRw5pja8uGjKfeuops641CFzrhqe1Pb169ZKdO3eGzO8zAShAf/T0D96yZcv8mht0W9v/cfq6du1q/ufwvaZaLap9ezzXVB/1f0D9g+jx8ccfm2uv/1rxHKPD7bW92kP/5aj/Uk9KSpKmTvuXa/jRphi9NnpdfenvbVRUlN911v5R2tbve521acc3bOo11D9S2rzjOcb3HJ5jnP77r7+LJSUlXOcGNGbMGHOdtKbNs2g/QO2j4lnnWjc8nV7ku+++M9OShMzvc4N0pUadhsHrCKWFCxea0UlTp041w+B9e8Cj+igOHR6pi/6qzps3z6zv3bvXOwxer+Fbb71lbdiwwbr66qtrHAY/ZMgQa/Xq1dbnn39uRoX4DoPX0Qo6DP4Xv/iFGY6sPycddumUYfDTpk0zUwksX77cbzhrUVGR33BWHRr/8ccfm+GsI0aMMMuJw1kvueQSM5Reh6i2adOmxuGsd999txkNMn/+fEcNGVb33XefGV23e/du8/uq2zoi8YMPPjDPc50bj+8oMMW1rr9f//rX5u+G/j5/8cUXZji7DmPXkaShco0JQAGkcxjoL4TOB6TD4nVuGtTuk08+McHnxGXy5MneofAzZ840AUbD5ZgxY8z8Kr6OHj1qAk/z5s3N8MrrrrvOBCtfOofQqFGjzDnat29vgpVT1HR9ddG5gTw0UP7qV78yQ7b1j9EPfvADE5J87dmzx7r00kvNHEr6R1D/OJaVlVX7eQ4ePNj8/nfr1s3vPZzg+uuvtzp37mw+v/6h199XT/hRXOfABSCudf3pcPR27dqZz65/N3V7586dIXWNw/Q/DVOXBAAAEBroAwQAAByHAAQAAByHAAQAAByHAAQAAByHAAQAAByHAAQAAByHAAQAAByHAAQgZC1fvlzCwsKq3XPodDz44IMyePBgCVa//OUvZcKECXYXA2hyCEBACNMvRw0AjzzyiN/+N9980+zHqf3mN7/xu99QsAUOvVnqwoUL7S4G0OQQgIAQFxsbK3/84x8lOztbQoHeJTqYNG/eXFq3bh20nzMxMdHcaRtAwyIAASFu7Nix0rZtW5k7d+5pNfM8+eST0qVLl2o1Hw8//LCkpqaaL92HHnpIysvL5e6775ZWrVpJhw4d5O9//7vfefbv3y8/+clPzPF6zNVXXy179uypdt4//OEPkpaWJr179zb79U7QF110kTRr1swEkKlTp5o7Sp/Mu+++K7169TKvufDCC/3ex+Pzzz+XCy64wBzTsWNHuf3226WwsLBO10bXFy1aJG+99ZapQdNFm9nq8zlfeuklcwfyFi1amJ/Tz372M787YKtNmzbJFVdcYe6Ercdp+fXO2r7n9dC7x+tnSklJMeF31KhR8tVXX1VrFtRaLX3fuLg4GTlypLkbty/9jGeffbY5R7du3eR3v/ud+VkrvUOSXotOnTpJTEyM+Tz6nkBTQgACQlxERIQJLU8//bQcOHCgXuf6+OOP5dChQ/Lpp5/KvHnzZPbs2eaLOSkpSVavXi0333yz3HTTTd73KSsrk3Hjxpkv7c8++0y++OILU6Myfvx4vxoQ/TLWL+APP/xQ/vvf/5pAoq/T8+qX96uvviofffSR3HrrrbWWTQPID3/4Q7nyyivlm2++kRtvvFHuu+8+v2M0NOh7/+hHP5INGzbIkiVLTCA62XlPbA7TkKPnOHz4sFk0PJzp5/Rco9///veyfv160zSpoUlDjcfBgwfle9/7ngkaev3Xrl0r119/vTeMnOiee+6Rf//73yaorVu3Tnr06GHKduzYMb/j7r//fnn88cfl66+/lsjISHNOD/0MkyZNkjvuuEM2b94szz33nGlm0/Cm9PxPPPGE2b9jxw5T7gEDBtTpGgIho8Fuqwog4CZPnmxdffXVZv28884zdxxXb7zxhrmru8fs2bOtQYMG+b32iSeeMHcn9z2XbrtcLu++3r17WxdccIF3u7y83IqPj7deeeUVs/3SSy+ZY9xut/eYkpISc3fn999/33ve1NRUs9/jL3/5i7lLdEFBgXffO++8Y4WHh1tHjhyp8bPOmDHD6tu3r9++e++913zO7Oxss33DDTdYU6dO9Tvms88+M+fVu1PX5MRr43tNPc70c9bkq6++MmXOz8/3fq6uXbtapaWlNR7vWx69XlFRUdY//vEP7/P6urS0NOvRRx/13j1bz//RRx/5XVvd57kGeif6hx9+uNpn1Lt7q8cff9zq1atXrWUCmgJqgIAmQvsBaa3Ali1bzvgc/fr1k/Dwqj8L2hTm+y9/rW3S5ipPE47WauzcudPUjGiNiC7aPFRcXOxtwlF6jujoaO+2lnHQoEESHx/v3Xf++eeL2+2u1lTj+5rhw4f77RsxYoTftpZHazI8ZdFFa0f0vLt37z7j63Kmn1NpjY7WWmlzkr5+9OjRZv++ffvMo9ZmaZNXVFTUKcuh76U1SnqtPPR15557brWf+8CBA73r7dq1M4++Pzdt3vS9TlOmTDE1XkVFRfLjH/9Yjh8/bprGdP8bb7xRa40UEKoi7S4AgIahzSj6ZT9jxgy/JhaloUb7dfjSL9ITnfglrH1JatqngUJpn52hQ4fKP/7xj2rnatOmjXfdN+g0Ji2PNtHV1F9FA0h9znsmn9PT1KeLvlaP1eCj256mM+2r1Bh8f26eEYG+Pzft86NNiifSPkHad0qDqDZLanPer371K3nsscdkxYoVdQpqQCggAAFNiA6H1w69ng64HvrFe+TIEROCPF+GWvNQX9qJVvvZaIdc7cBbV3369DE1NRoQPKFB+9VoUDux7L6vefvtt/32ffnll9XKo31atF/MmdIaHJfL1SCfc+vWrXL06FHzc9FQobRPzok1NVpzp4H0VOGie/fupnx6rTp37mz26eu0H9Wdd95Z53Lp59GAc7LrpMFMa650ueWWW+Sss84yHdf1tUBTQBMY0IRoE8y1114rTz31lN/+73//+5KZmSmPPvqoaUaZP3++vPfee/V+P32v5ORkMyJKO9ZqM5OOQtIamJN1yNbXaU3D5MmTZePGjfLJJ5/IbbfdJr/4xS9Ms1tNtAO2dsjVEWn65f3Pf/6z2vw49957r6xcudJ0etaAp8fraKe6doJWOjJOO1Dre2RlZZmAcaafU2udNLBoB/Vdu3aZAKcdon1p2fLy8uSaa64x4UjLrCPHamoK1LA4bdo0cw2WLl1qwp42UWmz1Q033FDnzzhr1ix58cUXTS2QjkDT5rPFixfLAw88YJ7X6/q3v/3N/Gy03C+//LIJRJ7QBTQFBCCgidG+HZ6mDt/akz//+c8m+GjfmzVr1pgRT/WlQ6x1xJh+0Wtzir6PfhFr35iT1ZTo695//30zcmnYsGHyf//3fzJmzBh55plnan2NvoeOTtIRSfoZFixYYEa/nVibos0027dvN/1qhgwZYr7sdRh3XWmg0FooHUKuNWda23Kmn1Nfr2FCR7n17dvX1AT9v//3//yO0T5VOvpLm6W0f5A2tT3//PO11gbpOXSUm4ZFrY3Rvkl6LXVEXV1pE5yOUvvggw/M9T/vvPPMqC9PwNGh/loG7Wuk11Sbwv7zn/80ynxJgF3CtCe0be8OAABgA2qAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACA4xCAAACAOM3/B02OZ0vYDAmfAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Elegir algun valor para alpha (probar varias alternativas)\n",
        "alpha = 0.01\n",
        "num_iters = 5000\n",
        "\n",
        "# inicializa theta y ejecuta el descenso por el gradiente\n",
        "theta = np.zeros(9)\n",
        "theta, J_history = descensoGradiente(theta, X, y, alpha, num_iters)\n",
        "\n",
        "# Grafica la convergencia del costo\n",
        "pyplot.plot(np.arange(len(J_history)), J_history, lw=2)\n",
        "pyplot.xlabel('Numero de iteraciones')\n",
        "pyplot.ylabel('Costo J')\n",
        "\n",
        "# Muestra los resultados del descenso por el gradiente\n",
        "print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCMi6DgUjA0O",
        "outputId": "925652d7-03df-4b9b-c6cd-f23b302bb70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paciente diagnosticado (usando el descenso por el gradiente): 0\n"
          ]
        }
      ],
      "source": [
        "# verificar si tiene o no Diabetes\n",
        "X_array = [1, 2,\t108,\t70,\t30,\t0,\t40,\t0.53,\t54] # Datos de Prueba\n",
        "X_array[1:] = (X_array[1:] - mu) / sigma\n",
        "prueba = sigmoid(np.dot(X_array, theta))   # Se debe cambiar esto\n",
        "\n",
        "print('Paciente diagnosticado (usando el descenso por el gradiente): {:.0f}'.format(prueba))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C2Qew-sRJVNj"
      },
      "outputs": [],
      "source": [
        "def costFunction(theta, X, y):\n",
        "    # Inicializar algunos valores utiles\n",
        "    m = y.size  # numero de ejemplos de entrenamiento\n",
        "\n",
        "    J = 0\n",
        "    grad = np.zeros(theta.shape)\n",
        "\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
        "    grad = (1 / m) * (h - y).dot(X)\n",
        "\n",
        "    return J, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ioLXhR5JVNj"
      },
      "source": [
        "Se prueba la funcion `costFunction` utilizando dos casos de prueba para $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt3g355DvVkq",
        "outputId": "8a6f8b2f-f86b-4c59-cd1f-d8213f6c86eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.6931471805599453),\n",
              " array([ 0.158     , -0.10646833, -0.21746571, -0.03603296, -0.03607194,\n",
              "        -0.0573637 , -0.13127295, -0.07374661, -0.11219516]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost, grad = costFunction(np.zeros(9), X, y)\n",
        "cost, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Fe_xEZJVNj",
        "outputId": "207de52b-173d-4973-ec63-dc794057dcee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Costo en theta inicial (zeros): 0.693\n",
            "[ 0.158      -0.10646833 -0.21746571 -0.03603296 -0.03607194 -0.0573637\n",
            " -0.13127295 -0.07374661 -0.11219516]\n",
            "Gradiente en theta inicial (zeros):\n",
            "\t[0.1580, -0.1065, -0.2175, -0.0360, -0.0361, -0.0574, -0.1313, -0.0737, -0.1122]\n"
          ]
        }
      ],
      "source": [
        "# Inicializacion de parametros de ajuste\n",
        "initial_theta = np.zeros(n+1)\n",
        "print(initial_theta)\n",
        "cost, grad = costFunction(initial_theta, X, y)\n",
        "\n",
        "print('Costo en theta inicial (zeros): {:.3f}'.format(cost))\n",
        "print(grad)\n",
        "print('Gradiente en theta inicial (zeros):')\n",
        "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AeXffwpJVNj"
      },
      "source": [
        "#### Parámetros de aprendizaje usando `scipy.optimize`\n",
        "\n",
        "En el codigo anterior se encontró los parámetros óptimos de un modelo de regresión lineal al implementar el descenso de gradiente. Se implemento una función de costo y se calculó su gradiente, utilizando el algoritmo del descenso por el gradiente.\n",
        "\n",
        "En lugar de realizar los pasos del descenso por el gradiente, se utilizará el [módulo `scipy.optimize`] (https://docs.scipy.org/doc/scipy/reference/optimize.html). SciPy es una biblioteca de computación numérica para `python`. Proporciona un módulo de optimización para la búsqueda y minimización de raíces. A partir de `scipy 1.0`, la función` scipy.optimize.minimize` es el método a utilizar para problemas de optimización (tanto restringidos como no restringidos).\n",
        "\n",
        "For logistic regression, you want to optimize the cost function $J(\\theta)$ with parameters $\\theta$.\n",
        "Concretely, you are going to use `optimize.minimize` to find the best parameters $\\theta$ for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to `optimize.minimize` the following inputs:\n",
        "\n",
        "Para la regresión logística, se desea optimizar la función de costo $J(\\theta)$ con los parámetros $\\theta$.\n",
        "Concretamente, se va a utilizar `optimize.minimize` para encontrar los mejores parámetros $\\theta$ para la función de costo de regresión logística, dado un dataset fijo (de valores X e y). Se pasara a `optimize.minimize` las siguientes entradas:\n",
        "\n",
        "- `costFunction`: Una función de costo que, cuando se le da el dataset de entrenamiento y un $\\theta$ particular, calcula el costo de regresión logística y el gradiente con respecto a $\\theta$ para el dataset(X, y). Es importante tener en cuenta que solo se pasa el nombre de la función sin el paréntesis. Esto indica que solo proporcionamos una referencia a esta función y no evaluamos el resultado de esta función.\n",
        "- `initial_theta`: Los valores iniciales de los parametros que se tratan de optimizar.\n",
        "- `(X, y)`: Estos son argumentos adicionales a la funcion de costo.\n",
        "- `jac`: Indicación si la función de costo devuelve el jacobiano (gradiente) junto con el valor de costo. (True)\n",
        "- `method`: Método / algoritmo de optimización a utilizar\n",
        "- `options`: Opciones adicionales que pueden ser específicas del método de optimización específico. Solo se indica al algoritmo el número máximo de iteraciones antes de que termine.\n",
        "\n",
        "Si se ha completado la `costFunction` correctamente,`optimize.minimize` convergerá en los parámetros de optimización correctos y devolverá los valores finales del costo y $\\theta$ en un objeto de clase.\n",
        "\n",
        "Al usar `optimize.minimize`, no se tuvo que escribir ningún bucle ni establecer una tasa de aprendizaje como se hizo para el descenso de gradientes. Todo esto se hace mediante `optimize.minimize`: solo se necesita proporcionar una función que calcule el costo y el gradiente.\n",
        "\n",
        "A continuación, se tiene el código para llamar a `optimize.minimize` con los argumentos correctos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB2xAu03JVNj",
        "outputId": "e112d579-22e3-4e90-92ee-14fb4ffb6bbe",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Costo con un valor de theta encontrado por optimize.minimize: 0.479\n",
            "theta:\n",
            "\t[-0.885, 0.418, 1.081, -0.185, 0.008, -0.138, 0.632, 0.287, 0.152]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\GERMAN ZELAYA\\AppData\\Local\\Temp\\ipykernel_18740\\1628533777.py:7: OptimizeWarning: Unknown solver options: maxiter\n",
            "  res = optimize.minimize(costFunction,\n"
          ]
        }
      ],
      "source": [
        "# Establecer las opciones para optimize.minimize\n",
        "options= {'maxiter': 1000}\n",
        "\n",
        "# revisar la documentacion de scipy's optimize.minimize para mayor descripcion de los parametros\n",
        "# La funcion devuekve un objeto `OptimizeResult`\n",
        "# Se utiliza el algoritmo de Newton truncado para la optimización.\n",
        "res = optimize.minimize(costFunction,\n",
        "                        initial_theta,\n",
        "                        (X, y),\n",
        "                        jac=True,\n",
        "                        method='TNC',\n",
        "                        options=options)\n",
        "\n",
        "# la propiedad fun del objeto devuelto por `OptimizeResult`\n",
        "# contiene el valor del costFunction de un theta optimizado\n",
        "cost = res.fun\n",
        "\n",
        "# Theta optimizada esta en la propiedad x\n",
        "theta = res.x\n",
        "\n",
        "# Imprimir theta en la pantalla\n",
        "print('Costo con un valor de theta encontrado por optimize.minimize: {:.3f}'.format(cost))\n",
        "\n",
        "print('theta:')\n",
        "print('\\t[{:.3f}, {:.3f}, {:.3f}, {:.3f}, {:.3f}, {:.3f}, {:.3f}, {:.3f}, {:.3f}]'.format(*theta))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
